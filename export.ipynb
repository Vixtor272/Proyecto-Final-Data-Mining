{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abe62cde",
   "metadata": {},
   "source": [
    "# Exportar Dataset para Machine Learning\n",
    "Este script extrae la tabla final `analytics.daily_features` y la guarda como \n",
    "un archivo CSV. Este archivo CSV será el **input oficial** para tus futuros \n",
    "modelos de ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a37006b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leyendo datos desde Postgres...\n",
      "Dimensiones del Dataset: (4458, 15)\n",
      "Columnas: ['date', 'ticker', 'year', 'month', 'day_of_week', 'open', 'close', 'high', 'low', 'volume', 'return_close_open', 'return_prev_close', 'volatility_20_days', 'run_id', 'ingested_at_utc']\n",
      "\n",
      "--- Primeras 5 filas ---\n",
      "         date ticker  year  month  day_of_week        open       close  \\\n",
      "0  2020-01-02    GLD  2020      1            3  143.860001  143.949997   \n",
      "1  2020-01-02    QQQ  2020      1            3  214.399994  216.160004   \n",
      "2  2020-01-02    SPY  2020      1            3  323.540009  324.869995   \n",
      "3  2020-01-03    GLD  2020      1            4  145.750000  145.860001   \n",
      "4  2020-01-03    QQQ  2020      1            4  213.300003  214.179993   \n",
      "\n",
      "         high         low    volume  return_close_open  return_prev_close  \\\n",
      "0  144.210007  143.399994   7733800           0.000626           0.000000   \n",
      "1  216.160004  213.979996  30969400           0.008209           0.000000   \n",
      "2  324.890015  322.529999  59151200           0.004111           0.000000   \n",
      "3  146.320007  145.399994  12272800           0.000755           0.013269   \n",
      "4  215.470001  213.279999  27518900           0.004126          -0.009160   \n",
      "\n",
      "   volatility_20_days     run_id            ingested_at_utc  \n",
      "0                 0.0  batch_001 2025-12-06 16:26:46.671433  \n",
      "1                 0.0  batch_001 2025-12-06 16:26:34.794553  \n",
      "2                 0.0  batch_001 2025-12-06 16:26:20.697858  \n",
      "3                 0.0  batch_001 2025-12-06 16:26:46.671433  \n",
      "4                 0.0  batch_001 2025-12-06 16:26:34.794553  \n",
      "\n",
      "--- Conteo de Nulos ---\n",
      "return_close_open     0\n",
      "return_prev_close     0\n",
      "volatility_20_days    0\n",
      "dtype: int64\n",
      "\n",
      "Archivo guardado exitosamente en: ./dataset_ml_features.csv\n",
      "Ahora puedes usar este archivo para entrenar tus modelos.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# 1. Configurar conexión \n",
    "PG_USER = os.getenv('PG_USER', 'admin')\n",
    "PG_PASSWORD = os.getenv('PG_PASSWORD', 'admin_password')\n",
    "PG_HOST = os.getenv('PG_HOST', 'postgres')\n",
    "PG_DB = os.getenv('PG_DB', 'trading_db')\n",
    "\n",
    "db_url = f\"postgresql://{PG_USER}:{PG_PASSWORD}@{PG_HOST}:5432/{PG_DB}\"\n",
    "engine = create_engine(db_url)\n",
    "\n",
    "# 2. Descargar toda la tabla\n",
    "# Ordenamos por fecha y ticker para mantener el orden temporal, vital en series de tiempo.\n",
    "query = \"SELECT * FROM analytics.daily_features ORDER BY date ASC, ticker ASC\"\n",
    "\n",
    "print(\"Leyendo datos desde Postgres...\")\n",
    "df_full = pd.read_sql(query, engine)\n",
    "\n",
    "print(f\"Dimensiones del Dataset: {df_full.shape}\")\n",
    "print(\"Columnas:\", df_full.columns.tolist())\n",
    "\n",
    "# 3. Visualizar los datos\n",
    "print(\"\\n--- Primeras 5 filas ---\")\n",
    "print(df_full.head())\n",
    "\n",
    "# Verificar que no tenemos nulos críticos en los features que usaremos\n",
    "print(\"\\n--- Conteo de Nulos ---\")\n",
    "print(df_full[['return_close_open', 'return_prev_close','volatility_20_days']].isnull().sum())\n",
    "\n",
    "# 4. Guardar a CSV\n",
    "output_dir = \"./\" \n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "output_file = os.path.join(output_dir, \"dataset_ml_features.csv\")\n",
    "\n",
    "# Guardar\n",
    "df_full.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"\\nArchivo guardado exitosamente en: {output_file}\")\n",
    "print(\"Ahora puedes usar este archivo para entrenar tus modelos.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
