{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5be65394",
   "metadata": {},
   "source": [
    "# Proyecto Final Primera Parte: Ingesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6a215a",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ace85543",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from sqlalchemy import create_engine, text\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fb2d9c",
   "metadata": {},
   "source": [
    "## Configuración y Variables de Entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703dc37d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando Tickers: ['SPY', 'QQQ', 'GLD']\n",
      "Rango: 2020-01-01 a 2025-12-01\n"
     ]
    }
   ],
   "source": [
    "# Leer variables \n",
    "PG_USER = os.getenv('PG_USER')\n",
    "PG_PASSWORD = os.getenv('PG_PASSWORD')\n",
    "PG_HOST = os.getenv('PG_HOST')\n",
    "PG_PORT = os.getenv('PG_PORT')\n",
    "PG_DB = os.getenv('PG_DB')\n",
    "PG_SCHEMA = os.getenv('PG_SCHEMA_RAW')\n",
    "\n",
    "# Obtener Tickers y fechas\n",
    "TICKERS = os.getenv('TICKERS').split(',')\n",
    "START_DATE = os.getenv('START_DATE')\n",
    "END_DATE = os.getenv('END_DATE')\n",
    "\n",
    "print(f\"Procesando Tickers: {TICKERS}\")\n",
    "print(f\"Rango: {START_DATE} a {END_DATE}\")\n",
    "\n",
    "# Crear string de conexión\n",
    "db_url = f\"postgresql://{PG_USER}:{PG_PASSWORD}@{PG_HOST}:{PG_PORT}/{PG_DB}\"\n",
    "engine = create_engine(db_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a187f48",
   "metadata": {},
   "source": [
    "## Función de Descarga y Transformación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c59fb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ticker(ticker):\n",
    "    print(f\"Descargando {ticker}...\")\n",
    "    # Descarga desde Yahoo Finance\n",
    "    df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False, auto_adjust=False)\n",
    "    \n",
    "    if df.empty:\n",
    "        print(f\"No se encontraron datos para {ticker}\")\n",
    "        return None\n",
    "\n",
    "    # Si yfinance devuelve columnas MultiIndex, \n",
    "    # extraemos solo el primer nivel ('Close').\n",
    "    if isinstance(df.columns, pd.MultiIndex):\n",
    "        df.columns = df.columns.get_level_values(0)\n",
    "\n",
    "    # Limpieza básica\n",
    "    df = df.reset_index()\n",
    "    \n",
    "    # Estandarizar nombres de columnas a minúsculas y sin espacios\n",
    "    df.columns = [str(c).lower().replace(' ', '_') for c in df.columns]\n",
    "    \n",
    "    # Renombrar 'date' si viene como 'datetime'\n",
    "    if 'date' not in df.columns and 'datetime' in df.columns:\n",
    "        df.rename(columns={'datetime': 'date'}, inplace=True)\n",
    "        \n",
    "    # Asegurar columnas de metadatos\n",
    "    df['ticker'] = ticker\n",
    "    df['run_id'] = 'batch_manual_01'\n",
    "    df['ingested_at_utc'] = datetime.utcnow()\n",
    "    df['source_name'] = 'yfinance'\n",
    "    \n",
    "    # Mapeo de columnas para asegurar match con DB\n",
    "    cols_map = {\n",
    "        'date': 'date',\n",
    "        'open': 'open',\n",
    "        'high': 'high',\n",
    "        'low': 'low',\n",
    "        'close': 'close',\n",
    "        'adj_close': 'adj_close',\n",
    "        'volume': 'volume',\n",
    "        'ticker': 'ticker',\n",
    "        'run_id': 'run_id',\n",
    "        'ingested_at_utc': 'ingested_at_utc',\n",
    "        'source_name': 'source_name'\n",
    "    }\n",
    "    \n",
    "    # Filtrar solo columnas existentes en el DF que coincidan con el mapa\n",
    "    available_cols = [c for c in cols_map.keys() if c in df.columns]\n",
    "    df = df[available_cols].rename(columns=cols_map)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c59c3a",
   "metadata": {},
   "source": [
    "## Carga a Base de Datos (Esquema RAW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b34e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descargando SPY...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VICTOR\\AppData\\Local\\Temp\\ipykernel_7056\\3856562998.py:30: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  df['ingested_at_utc'] = datetime.utcnow()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardadas 1486 filas para SPY\n",
      "Descargando QQQ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VICTOR\\AppData\\Local\\Temp\\ipykernel_7056\\3856562998.py:30: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  df['ingested_at_utc'] = datetime.utcnow()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardadas 1486 filas para QQQ\n",
      "Descargando GLD...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VICTOR\\AppData\\Local\\Temp\\ipykernel_7056\\3856562998.py:30: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  df['ingested_at_utc'] = datetime.utcnow()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardadas 1486 filas para GLD\n",
      "CSV consolidado guardado: raw_prices_daily.csv\n"
     ]
    }
   ],
   "source": [
    "# Limpiar tabla antes de insertar para evitar errores de llave primaria en pruebas\n",
    "try:\n",
    "    with engine.begin() as conn:\n",
    "        for ticker in TICKERS:\n",
    "            conn.execute(text(f\"DELETE FROM {PG_SCHEMA}.prices_daily WHERE ticker = :ticker\"), {'ticker': ticker})\n",
    "except Exception as e:\n",
    "    print(f\"Advertencia al limpiar tabla (puede estar vacía): {e}\")\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for ticker in TICKERS:\n",
    "    try:\n",
    "        df_result = process_ticker(ticker)\n",
    "        \n",
    "        if df_result is not None:\n",
    "            dfs.append(df_result)\n",
    "\n",
    "            df_result.to_sql(\n",
    "                'prices_daily',\n",
    "                engine,\n",
    "                schema=PG_SCHEMA,\n",
    "                if_exists='append',\n",
    "                index=False\n",
    "            )\n",
    "            print(f\"Guardadas {len(df_result)} filas para {ticker}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error procesando {ticker}: {e}\")\n",
    "\n",
    "# Guardar CSV consolidado \n",
    "if dfs:\n",
    "    df_all = pd.concat(dfs, ignore_index=True)\n",
    "    df_all.to_csv(\"raw_prices_daily.csv\", index=False)\n",
    "    print(\"CSV consolidado guardado: raw_prices_daily.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ab8433",
   "metadata": {},
   "source": [
    "## Verificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3cf527a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resumen de Ingesta:\n",
      "('GLD', 1486, datetime.date(2020, 1, 2), datetime.date(2025, 11, 28))\n",
      "('SPY', 1486, datetime.date(2020, 1, 2), datetime.date(2025, 11, 28))\n",
      "('QQQ', 1486, datetime.date(2020, 1, 2), datetime.date(2025, 11, 28))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    with engine.connect() as conn:\n",
    "        query = text(f\"SELECT ticker, COUNT(*) as cnt, MIN(date) as min_d, MAX(date) as max_d FROM {PG_SCHEMA}.prices_daily GROUP BY ticker\")\n",
    "        result = conn.execute(query)\n",
    "        print(\"\\nResumen de Ingesta:\")\n",
    "        for row in result:\n",
    "            print(row)\n",
    "except Exception as e:\n",
    "    print(f\"Error en verificación: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
